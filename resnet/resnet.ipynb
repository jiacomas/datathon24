{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and import of the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV con las características\n",
    "df = pd.read_csv('wide_data.csv')\n",
    "\n",
    "# Definir la carpeta donde se encuentran las imágenes\n",
    "image_folder = '../archive/images/images'\n",
    "\n",
    "df_subset = df.sample(n=5000, random_state=42)\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    try:\n",
    "        img = load_img(img_path, target_size=(224, 224))  # Redimensionar la imagen\n",
    "        img_array = img_to_array(img)  # Convertir la imagen a array\n",
    "        img_array = preprocess_input(img_array)  # Preprocesar la imagen para ResNet50\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar la imagen {img_path}: {e}\")\n",
    "        return None  # Retornar None si la imagen no se carga correctamente\n",
    "\n",
    "# Función para cargar las etiquetas con valores nulos reemplazados por 0\n",
    "def load_labels(row):\n",
    "    label = [\n",
    "        row['silhouette_type'] if pd.notna(row['silhouette_type']) else 0,\n",
    "        row['waist_type'] if pd.notna(row['waist_type']) else 0,\n",
    "        row['neck_lapel_type'] if pd.notna(row['neck_lapel_type']) else 0,\n",
    "        row['sleeve_length_type'] if pd.notna(row['sleeve_length_type']) else 0,\n",
    "        row['toecap_type'] if pd.notna(row['toecap_type']) else 0,\n",
    "        row['closure_placement'] if pd.notna(row['closure_placement']) else 0,\n",
    "        row['cane_height_type'] if pd.notna(row['cane_height_type']) else 0,\n",
    "        row['heel_shape_type'] if pd.notna(row['heel_shape_type']) else 0,\n",
    "        row['knit_structure'] if pd.notna(row['knit_structure']) else 0,\n",
    "        row['length_type'] if pd.notna(row['length_type']) else 0,\n",
    "        row['woven_structure'] if pd.notna(row['woven_structure']) else 0\n",
    "    ]\n",
    "    return label\n",
    "\n",
    "# Función que convierte las imágenes y etiquetas en tensores adecuados para el entrenamiento\n",
    "def image_data_generator(df, image_folder, batch_size=32):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        img_id = row['des_filename']\n",
    "        img_path = os.path.join(image_folder, f\"{img_id}\")\n",
    "\n",
    "        try:\n",
    "            img_array = preprocess_image(img_path)\n",
    "            images.append(img_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar la imagen {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        label = load_labels(row)\n",
    "        labels.append(label)\n",
    "\n",
    "        # Cuando se alcanza el tamaño del lote, se devuelve el lote\n",
    "        if len(images) == batch_size:\n",
    "            labels_one_hot = [to_categorical(np.array(labels)[:, i], num_classes=10000) for i in range(len(labels[0]))]\n",
    "            yield np.array(images), tuple(labels_one_hot)  # Devolvemos una tupla\n",
    "            images = []\n",
    "            labels = []\n",
    "\n",
    "    # Devolver el último lote si hay imágenes restantes\n",
    "    if len(images) > 0:\n",
    "        labels_one_hot = [to_categorical(np.array(labels)[:, i], num_classes=10000) for i in range(len(labels[0]))]\n",
    "        yield np.array(images), tuple(labels_one_hot)  # Devolvemos una tupla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo base de ResNet50 sin la capa superior\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Agregar las capas para la predicción de las características\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Añadir una capa de salida para cada característica\n",
    "output_silhouette = Dense(10000, activation='softmax', name='silhouette_type')(x)\n",
    "output_waist = Dense(10000, activation='softmax', name='waist_type')(x)\n",
    "output_neck_lapel = Dense(10000, activation='softmax', name='neck_lapel_type')(x)\n",
    "output_sleeve_length = Dense(10000, activation='softmax', name='sleeve_length_type')(x)\n",
    "output_toecap = Dense(10000, activation='softmax', name='toecap_type')(x)\n",
    "output_closure = Dense(10000, activation='softmax', name='closure_placement')(x)\n",
    "output_cane_height = Dense(10000, activation='softmax', name='cane_height_type')(x)\n",
    "output_heel_shape = Dense(10000, activation='softmax', name='heel_shape_type')(x)\n",
    "output_knit = Dense(10000, activation='softmax', name='knit_structure')(x)\n",
    "output_length = Dense(10000, activation='softmax', name='length_type')(x)\n",
    "output_woven = Dense(10000, activation='softmax', name='woven_structure')(x)\n",
    "\n",
    "# Crear el modelo con múltiples salidas\n",
    "model = Model(inputs=base_model.input, outputs=[output_silhouette, output_waist, output_neck_lapel,\n",
    "                                                output_sleeve_length, output_toecap, output_closure,\n",
    "                                                output_cane_height, output_heel_shape, output_knit,\n",
    "                                                output_length, output_woven])\n",
    "\n",
    "# Congelar las capas del modelo base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'] * 11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - cane_height_type_accuracy: 0.9213 - cane_height_type_loss: 1.1015 - closure_placement_accuracy: 0.5152 - closure_placement_loss: 1.9447 - heel_shape_type_accuracy: 0.8980 - heel_shape_type_loss: 1.2652 - knit_structure_accuracy: 0.7748 - knit_structure_loss: 1.4823 - length_type_accuracy: 0.4357 - length_type_loss: 2.3849 - loss: 20.3014 - neck_lapel_type_accuracy: 0.3825 - neck_lapel_type_loss: 2.9032 - silhouette_type_accuracy: 0.3776 - silhouette_type_loss: 2.7604 - sleeve_length_type_accuracy: 0.5495 - sleeve_length_type_loss: 1.8657 - toecap_type_accuracy: 0.8965 - toecap_type_loss: 1.1355 - waist_type_accuracy: 0.7484 - waist_type_loss: 1.5196 - woven_structure_accuracy: 0.4385 - woven_structure_loss: 1.9383\n",
      "Epoch 2/5\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243us/step - cane_height_type_accuracy: 1.0000 - cane_height_type_loss: 7.7494e-04 - closure_placement_accuracy: 0.8750 - closure_placement_loss: 0.1746 - heel_shape_type_accuracy: 1.0000 - heel_shape_type_loss: 0.0483 - knit_structure_accuracy: 1.0000 - knit_structure_loss: 0.0531 - length_type_accuracy: 0.7500 - length_type_loss: 0.5940 - loss: 5.6801 - neck_lapel_type_accuracy: 0.7500 - neck_lapel_type_loss: 0.3719 - silhouette_type_accuracy: 0.6250 - silhouette_type_loss: 0.6216 - sleeve_length_type_accuracy: 0.8750 - sleeve_length_type_loss: 0.2135 - toecap_type_accuracy: 1.0000 - toecap_type_loss: 0.0437 - waist_type_accuracy: 0.8750 - waist_type_loss: 0.3065 - woven_structure_accuracy: 0.2500 - woven_structure_loss: 0.4302\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 06:02:13.253216: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 2s/step - cane_height_type_accuracy: 0.9959 - cane_height_type_loss: 0.0154 - closure_placement_accuracy: 0.6836 - closure_placement_loss: 0.8526 - heel_shape_type_accuracy: 0.9869 - heel_shape_type_loss: 0.0550 - knit_structure_accuracy: 0.8530 - knit_structure_loss: 0.4435 - length_type_accuracy: 0.6041 - length_type_loss: 1.1374 - loss: 7.5121 - neck_lapel_type_accuracy: 0.5455 - neck_lapel_type_loss: 1.4359 - silhouette_type_accuracy: 0.4673 - silhouette_type_loss: 1.5191 - sleeve_length_type_accuracy: 0.7811 - sleeve_length_type_loss: 0.6403 - toecap_type_accuracy: 0.9887 - toecap_type_loss: 0.0344 - waist_type_accuracy: 0.8393 - waist_type_loss: 0.4299 - woven_structure_accuracy: 0.5988 - woven_structure_loss: 0.9485\n",
      "Epoch 4/5\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108us/step - cane_height_type_accuracy: 1.0000 - cane_height_type_loss: 2.9969e-04 - closure_placement_accuracy: 1.0000 - closure_placement_loss: 0.1470 - heel_shape_type_accuracy: 1.0000 - heel_shape_type_loss: 0.0051 - knit_structure_accuracy: 1.0000 - knit_structure_loss: 0.0465 - length_type_accuracy: 0.7500 - length_type_loss: 0.4636 - loss: 4.0165 - neck_lapel_type_accuracy: 0.8750 - neck_lapel_type_loss: 0.2587 - silhouette_type_accuracy: 0.7500 - silhouette_type_loss: 0.4722 - sleeve_length_type_accuracy: 1.0000 - sleeve_length_type_loss: 0.1393 - toecap_type_accuracy: 1.0000 - toecap_type_loss: 0.0032 - waist_type_accuracy: 1.0000 - waist_type_loss: 0.1516 - woven_structure_accuracy: 0.8750 - woven_structure_loss: 0.3336  \n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 06:06:57.265510: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 2s/step - cane_height_type_accuracy: 0.9974 - cane_height_type_loss: 0.0067 - closure_placement_accuracy: 0.7076 - closure_placement_loss: 0.7697 - heel_shape_type_accuracy: 0.9925 - heel_shape_type_loss: 0.0248 - knit_structure_accuracy: 0.8622 - knit_structure_loss: 0.3956 - length_type_accuracy: 0.6281 - length_type_loss: 1.0206 - loss: 6.6779 - neck_lapel_type_accuracy: 0.6005 - neck_lapel_type_loss: 1.2389 - silhouette_type_accuracy: 0.5084 - silhouette_type_loss: 1.3633 - sleeve_length_type_accuracy: 0.8068 - sleeve_length_type_loss: 0.5692 - toecap_type_accuracy: 0.9932 - toecap_type_loss: 0.0219 - waist_type_accuracy: 0.8537 - waist_type_loss: 0.3793 - woven_structure_accuracy: 0.6243 - woven_structure_loss: 0.8879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f7b737c7760>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uso de tf.data para generar los lotes de imágenes y etiquetas\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: image_data_generator(df_subset, image_folder),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        (tf.TensorSpec(shape=(None, 10000), dtype=tf.float32),) * 11  # Definir las salidas como una tupla de tensores\n",
    "    )\n",
    ")\n",
    "\n",
    "# Ajustar el número de pasos por época y el tamaño de los lotes\n",
    "steps_per_epoch = len(df_subset) // 32\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model.fit(train_dataset, steps_per_epoch=steps_per_epoch, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "from PIL import UnidentifiedImageError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertencia: No se pudo cargar la imagen ../archive/images/images/88_49720742_67044470-56_.jpg\n"
     ]
    }
   ],
   "source": [
    "# Ruta de la carpeta de imágenes\n",
    "test_image_folder = '../archive/images/images'\n",
    "\n",
    "# Obtener la lista de archivos en la carpeta\n",
    "all_images = sorted(\n",
    "    [f for f in os.listdir(test_image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    ")[-5000:]  # Seleccionar las últimas 3500 imágenes\n",
    "\n",
    "# Función para cargar y preprocesar una imagen\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        img = load_img(image_path, target_size=(224, 224))  # Redimensionar\n",
    "        img_array = img_to_array(img)  # Convertir a array\n",
    "        img_array = preprocess_input(img_array)  # Preprocesar\n",
    "        return img_array\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"Advertencia: No se pudo cargar la imagen {image_path}\")\n",
    "        return None\n",
    "\n",
    "# Filtrar imágenes válidas\n",
    "test_images_arrays = []\n",
    "valid_image_paths = []\n",
    "\n",
    "for img in all_images:\n",
    "    img_path = os.path.join(test_image_folder, img)\n",
    "    processed_img = preprocess_image(img_path)\n",
    "    if processed_img is not None:\n",
    "        test_images_arrays.append(processed_img)\n",
    "        valid_image_paths.append(img)  # Solo guardar rutas de imágenes válidas\n",
    "\n",
    "test_images_arrays = np.array(test_images_arrays)\n",
    "\n",
    "# Crear el archivo CSV para guardar los resultados\n",
    "output_csv = 'predictions.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Realizar predicciones\n",
    "predictions = model.predict(test_images_arrays)\n",
    "\n",
    "# Crear el archivo CSV para guardar los resultados\n",
    "output_csv = 'predictions.csv'\n",
    "\n",
    "# Escribir los resultados en el archivo CSV\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['test_id', 'des_value'])  # Escribir cabecera del archivo CSV\n",
    "\n",
    "    for img_idx, img_id in enumerate(valid_image_paths):\n",
    "        img_name = os.path.splitext(img_id)[0]  # Obtener el ID de la imagen sin extensión\n",
    "        for output_idx, output_name in enumerate(model.output_names):\n",
    "            class_id = np.argmax(predictions[output_idx][img_idx])  # Clase más probable\n",
    "            writer.writerow([f\"{img_name}_{output_name}\", f\"{class_id:04d}\"])  # Escribir la fila con formato numérico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos, omitiendo la primera fila\n",
    "predictions = pd.read_csv('predictions.csv', header=None, names=['test_id', 'des_value'], skiprows=1)\n",
    "\n",
    "# Cargar los datos de atributos\n",
    "attributes_data = pd.read_csv('../archive/attribute_data.csv', header=None, \n",
    "                              names=['cod_modelo_color', 'attribute_name', 'cod_value', 'des_value'], skiprows=1)\n",
    "\n",
    "# Crear un diccionario de mapeo {cod_value: des_value} desde attribute_data\n",
    "value_to_description = attributes_data.set_index('cod_value')['des_value'].to_dict()\n",
    "\n",
    "# Función para limpiar 'test_id'\n",
    "def clean_test_id(test_id):\n",
    "    parts = test_id.split('__')\n",
    "    if len(parts) > 1 and '-' in parts[0]:\n",
    "        prefix, suffix = parts[0].split('-')[0].split('_')[:2]\n",
    "        attribute = parts[1]\n",
    "        return f\"{prefix}_{suffix}_{attribute}\"\n",
    "    return None\n",
    "\n",
    "# Aplicar la función para limpiar IDs\n",
    "predictions['cleaned_test_id'] = predictions['test_id'].apply(clean_test_id)\n",
    "\n",
    "# Filtrar filas con IDs válidos\n",
    "predictions = predictions[predictions['cleaned_test_id'].notnull()]\n",
    "\n",
    "# Mapear los valores de la columna 'des_value' a sus descripciones\n",
    "predictions['des_value'] = predictions['des_value'].map(value_to_description)\n",
    "\n",
    "# Eliminar filas duplicadas basadas en 'cleaned_test_id'\n",
    "unique_predictions = predictions.drop_duplicates(subset='cleaned_test_id', keep='first')\n",
    "\n",
    "# Guardar el archivo resultante en submission.csv\n",
    "unique_predictions[['cleaned_test_id', 'des_value']].to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
